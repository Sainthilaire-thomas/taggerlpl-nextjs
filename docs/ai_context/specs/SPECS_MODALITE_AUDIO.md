# üéôÔ∏è SPECS : Annotation Modalit√© Audio

## üéØ Vue d'Ensemble

**Objectif** : Permettre l'annotation automatique des paires en utilisant des LLM multimodaux (GPT-4o Audio) qui analysent les fichiers audio complets (texte + prosodie + ton + √©motion).

**Principe** : Au lieu de passer uniquement le verbatim texte au LLM, on lui envoie le fichier audio du turn client, permettant une analyse compl√®te incluant les indices prosodiques.

**B√©n√©fice Scientifique** : Mesurer si un LLM multimodal peut atteindre un accord similaire √† un humain √©coutant l'audio complet (hypoth√®se H5).

---

## üèóÔ∏è Architecture Globale

### Flux Annotation Audio

```
1. Fichiers audio originaux (appels complets)
   ‚Üì
2. Extraction segments audio (ffmpeg)
   ‚Üí client_turn_123.wav
   ‚Üí conseiller_turn_124.wav
   ‚Üì
3. Envoi fichier audio + prompt √† GPT-4o Audio
   ‚Üì
4. GPT-4o analyse texte + prosodie + ton
   ‚Üì
5. Retour tag + raisonnement
   ‚Üì
6. Sauvegarde annotation (annotator_type='llm_openai_audio')
```

---

## üóÑÔ∏è Base de Donn√©es

### Table turntagged (Existante) ‚úÖ

**Contient d√©j√† les timestamps** :

```sql
CREATE TABLE turntagged (
  turn_id INTEGER PRIMARY KEY,
  call_id TEXT,
  start_time FLOAT,      -- Timestamp d√©but (secondes)
  end_time FLOAT,        -- Timestamp fin (secondes)
  speaker TEXT,          -- 'client' ou 'conseiller'
  verbatim TEXT,
  ...
);
```

**Les timestamps sont disponibles** ‚Üí Aucune modification n√©cessaire !

---

### Table analysis_pairs (Existante) ‚úÖ

**R√©f√©rences les turn IDs** :

```sql
CREATE TABLE analysis_pairs (
  pair_id INTEGER PRIMARY KEY,
  call_id TEXT,
  client_turn_id INTEGER REFERENCES turntagged(turn_id),
  conseiller_turn_id INTEGER REFERENCES turntagged(turn_id),
  ...
);
```

**Jointure pour r√©cup√©rer timestamps** :
```sql
SELECT 
  ap.pair_id,
  tc.start_time as client_start,
  tc.end_time as client_end,
  tc.verbatim as client_verbatim,
  tco.start_time as conseiller_start,
  tco.end_time as conseiller_end
FROM analysis_pairs ap
JOIN turntagged tc ON tc.turn_id = ap.client_turn_id
JOIN turntagged tco ON tco.turn_id = ap.conseiller_turn_id
WHERE ap.pair_id = 3187;
```

---

### Table annotations (Existante) ‚úÖ

**Accepte d√©j√† le type 'llm_openai_audio'** :

```sql
-- Aucune modification n√©cessaire
INSERT INTO annotations VALUES (
  gen_random_uuid(),
  3187,                                      -- pair_id
  'llm_openai_audio',                        -- üÜï Type audio
  'GPT4o-audio_CharteY_B_v1.0.0',           -- üÜï ID audio
  'CLIENT_POSITIF',
  0.95,
  'Le client exprime un accord avec un ton enthousiaste...',
  '{"model": "gpt-4o-audio-preview", "audio_file": "client_turn_123.wav", "audio_duration_s": 3.2}',
  NOW(),
  ...
);
```

---

## üíª Services TypeScript

### AudioExtractionService.ts

**Extraction segments audio avec ffmpeg** :

```typescript
import ffmpeg from 'fluent-ffmpeg';
import { getSupabase } from '@/lib/supabaseClient';
import fs from 'fs';
import path from 'path';

export class AudioExtractionService {
  private static supabase = getSupabase();

  /**
   * Extraire segment audio pour une paire
   * @returns Chemins vers fichiers audio client et conseiller
   */
  static async extractPairAudio(
    pairId: number,
    originalAudioPath: string,
    outputDir: string
  ): Promise<{ clientPath: string; conseillerPath: string }> {
    
    // R√©cup√©rer timestamps depuis DB
    const { data, error } = await this.supabase
      .from('analysis_pairs')
      .select(`
        pair_id,
        call_id,
        client_turn_id,
        conseiller_turn_id,
        turntagged!client_turn_id (start_time, end_time),
        turntagged!conseiller_turn_id (start_time, end_time)
      `)
      .eq('pair_id', pairId)
      .single();

    if (error || !data) {
      throw new Error(`Pair ${pairId} not found: ${error?.message}`);
    }

    const clientTurn = (data.turntagged as any)[0];
    const conseillerTurn = (data.turntagged as any)[1];

    // Cr√©er dossier output si n√©cessaire
    if (!fs.existsSync(outputDir)) {
      fs.mkdirSync(outputDir, { recursive: true });
    }

    // Extraire turn client
    const clientPath = path.join(outputDir, `client_turn_${data.client_turn_id}.wav`);
    await this.extractSegment(
      originalAudioPath,
      clientTurn.start_time,
      clientTurn.end_time,
      clientPath
    );

    // Extraire turn conseiller
    const conseillerPath = path.join(outputDir, `conseiller_turn_${data.conseiller_turn_id}.wav`);
    await this.extractSegment(
      originalAudioPath,
      conseillerTurn.start_time,
      conseillerTurn.end_time,
      conseillerPath
    );

    return { clientPath, conseillerPath };
  }

  /**
   * Extraire segment audio avec ffmpeg
   */
  private static async extractSegment(
    inputPath: string,
    startTime: number,
    endTime: number,
    outputPath: string
  ): Promise<void> {
    
    // Ne pas r√©extraire si fichier existe d√©j√†
    if (fs.existsSync(outputPath)) {
      console.log(`Audio segment already exists: ${outputPath}`);
      return;
    }

    return new Promise((resolve, reject) => {
      ffmpeg(inputPath)
        .setStartTime(startTime)
        .setDuration(endTime - startTime)
        .output(outputPath)
        .audioCodec('pcm_s16le')    // WAV format
        .audioFrequency(16000)      // 16kHz (optimal pour speech)
        .audioChannels(1)           // Mono
        .on('end', () => {
          console.log(`Extracted: ${outputPath}`);
          resolve();
        })
        .on('error', (err) => {
          console.error(`Error extracting ${outputPath}:`, err);
          reject(err);
        })
        .run();
    });
  }

  /**
   * Batch extraction pour un call complet
   */
  static async extractCallAudio(
    callId: string,
    originalAudioPath: string,
    outputDir: string
  ): Promise<{ success: number; errors: number }> {
    
    // R√©cup√©rer toutes les paires du call
    const { data: pairs, error } = await this.supabase
      .from('analysis_pairs')
      .select('pair_id')
      .eq('call_id', callId);

    if (error || !pairs) {
      throw new Error(`Error fetching pairs: ${error?.message}`);
    }

    let successCount = 0;
    let errorCount = 0;

    for (const pair of pairs) {
      try {
        await this.extractPairAudio(pair.pair_id, originalAudioPath, outputDir);
        successCount++;
      } catch (err) {
        console.error(`Error extracting pair ${pair.pair_id}:`, err);
        errorCount++;
      }
    }

    return { success: successCount, errors: errorCount };
  }

  /**
   * Batch extraction pour tous les calls
   */
  static async extractAllAudio(
    audioBasePath: string,
    outputBaseDir: string
  ): Promise<{ totalCalls: number; totalPairs: number; errors: number }> {
    
    // R√©cup√©rer tous les calls distincts
    const { data: calls, error } = await this.supabase
      .from('analysis_pairs')
      .select('call_id')
      .order('call_id');

    if (error || !calls) {
      throw new Error(`Error fetching calls: ${error?.message}`);
    }

    const uniqueCalls = [...new Set(calls.map(c => c.call_id))];
    
    let totalPairs = 0;
    let totalErrors = 0;

    for (const callId of uniqueCalls) {
      const originalAudioPath = path.join(audioBasePath, `${callId}.wav`);
      const outputDir = path.join(outputBaseDir, callId);

      if (!fs.existsSync(originalAudioPath)) {
        console.warn(`Audio file not found: ${originalAudioPath}`);
        continue;
      }

      const result = await this.extractCallAudio(callId, originalAudioPath, outputDir);
      totalPairs += result.success;
      totalErrors += result.errors;
    }

    return {
      totalCalls: uniqueCalls.length,
      totalPairs,
      errors: totalErrors
    };
  }

  /**
   * Obtenir dur√©e audio
   */
  static async getAudioDuration(filePath: string): Promise<number> {
    return new Promise((resolve, reject) => {
      ffmpeg.ffprobe(filePath, (err, metadata) => {
        if (err) {
          reject(err);
        } else {
          resolve(metadata.format.duration || 0);
        }
      });
    });
  }
}
```

---

### OpenAIAudioAnnotationService.ts

**Annotation avec GPT-4o Audio** :

```typescript
import OpenAI from 'openai';
import fs from 'fs';
import { CharteRegistry } from './CharteRegistry';
import { AnnotationService } from './AnnotationService';

export interface AudioAnnotationResult {
  pair_id: number;
  tag: string;
  reasoning: string;
  confidence: number;
  audio_duration_s: number;
  processing_time_ms: number;
}

export class OpenAIAudioAnnotationService {
  private static openai = new OpenAI({
    apiKey: process.env.OPENAI_API_KEY,
  });

  /**
   * Annoter une paire avec l'audio
   */
  static async annotateWithAudio(
    pair: any,
    charteId: string,
    audioFilePath: string
  ): Promise<AudioAnnotationResult> {
    
    const startTime = Date.now();

    // Charger la charte
    const charte = await CharteRegistry.getCharteById(charteId);
    if (!charte) {
      throw new Error(`Charte ${charteId} not found`);
    }

    // V√©rifier fichier audio existe
    if (!fs.existsSync(audioFilePath)) {
      throw new Error(`Audio file not found: ${audioFilePath}`);
    }

    // Construire prompt avec instructions audio
    const prompt = this.buildAudioPrompt(charte, pair);

    try {
      // Lire le fichier audio
      const audioBuffer = fs.readFileSync(audioFilePath);
      const audioBase64 = audioBuffer.toString('base64');

      // Appel API GPT-4o Audio
      const response = await this.openai.chat.completions.create({
        model: "gpt-4o-audio-preview",
        modalities: ["text", "audio"],
        audio: { voice: "alloy", format: "wav" },
        messages: [
          {
            role: "system",
            content: "Vous √™tes un expert en analyse conversationnelle sp√©cialis√© dans l'annotation de conversations t√©l√©phoniques."
          },
          {
            role: "user",
            content: [
              {
                type: "text",
                text: prompt
              },
              {
                type: "input_audio",
                input_audio: {
                  data: audioBase64,
                  format: "wav"
                }
              }
            ]
          }
        ],
        temperature: 0.0,
        max_tokens: 500
      });

      const processingTime = Date.now() - startTime;

      // Parser r√©ponse
      const content = response.choices[0].message.content || '';
      const tag = this.extractTag(content, charte);
      const reasoning = this.extractReasoning(content);

      // Obtenir dur√©e audio
      const audioDuration = await this.getAudioDuration(audioFilePath);

      // Sauvegarder annotation
      await AnnotationService.saveAnnotation({
        pair_id: pair.pair_id,
        annotator_type: 'llm_openai_audio',
        annotator_id: `GPT4o-audio_${charteId}`,
        reaction_tag: tag,
        confidence: 0.95,
        reasoning: reasoning,
        annotation_context: {
          model: "gpt-4o-audio-preview",
          audio_file: audioFilePath,
          audio_duration_s: audioDuration,
          processing_time_ms: processingTime
        }
      });

      return {
        pair_id: pair.pair_id,
        tag,
        reasoning,
        confidence: 0.95,
        audio_duration_s: audioDuration,
        processing_time_ms: processingTime
      };

    } catch (error: any) {
      console.error(`Error annotating pair ${pair.pair_id} with audio:`, error);
      throw error;
    }
  }

  /**
   * Construire prompt sp√©cifique audio
   */
  private static buildAudioPrompt(charte: any, pair: any): string {
    const basePrompt = charte.prompt_template || '';
    
    const audioPrompt = `
${basePrompt}

üéôÔ∏è INSTRUCTIONS SP√âCIFIQUES AUDIO :

Vous allez entendre un extrait audio d'un CLIENT au t√©l√©phone avec un conseiller.

Analysez TOUS les √©l√©ments suivants :
1. CONTENU VERBAL : Les mots prononc√©s (texte)
2. PROSODIE : Le ton, l'intonation, le rythme
3. √âMOTION : Les indices √©motionnels (enthousiasme, d√©ception, h√©sitation, irritation)
4. CONTEXTE : Le tour de parole pr√©c√©dent et suivant

CONTEXTE CONVERSATIONNEL :
Tour pr√©c√©dent (conseiller) : ${pair.conseiller_verbatim || 'N/A'}
Tour suivant : ${pair.next1_verbatim || 'N/A'}

‚ö†Ô∏è IMPORTANT : 
- Le TON peut compl√®tement changer le sens d'un mot
- "oui" avec ton enthousiaste = POSITIF
- "oui" avec ton d√©pit√© = NEUTRE ou NEGATIF
- Priorisez la prosodie sur le texte si conflit

R√©pondez avec :
1. La cat√©gorie (CLIENT_POSITIF, CLIENT_NEGATIF, ou CLIENT_NEUTRE)
2. Votre raisonnement incluant analyse du TON et de l'√âMOTION
`;

    return audioPrompt;
  }

  /**
   * Extraire tag de la r√©ponse
   */
  private static extractTag(content: string, charte: any): string {
    const categories = Object.keys(charte.definition?.categories || {});
    
    for (const cat of categories) {
      if (content.includes(cat)) {
        return cat;
      }
    }
    
    throw new Error(`No valid tag found in response: ${content}`);
  }

  /**
   * Extraire raisonnement
   */
  private static extractReasoning(content: string): string {
    // Extraire texte apr√®s la cat√©gorie
    const lines = content.split('\n');
    const reasoningLines = lines.filter(l => 
      !l.includes('CLIENT_POSITIF') && 
      !l.includes('CLIENT_NEGATIF') && 
      !l.includes('CLIENT_NEUTRE') &&
      l.trim().length > 0
    );
    
    return reasoningLines.join(' ').trim();
  }

  /**
   * Obtenir dur√©e audio
   */
  private static async getAudioDuration(filePath: string): Promise<number> {
    const ffmpeg = require('fluent-ffmpeg');
    return new Promise((resolve, reject) => {
      ffmpeg.ffprobe(filePath, (err: any, metadata: any) => {
        if (err) reject(err);
        else resolve(metadata.format.duration || 0);
      });
    });
  }

  /**
   * Batch annotation avec rate limiting
   */
  static async annotateBatchWithAudio(
    pairs: any[],
    charteId: string,
    audioDirectory: string
  ): Promise<{
    success: number;
    errors: number;
    results: AudioAnnotationResult[];
    totalCost: number;
    totalDuration: number;
  }> {
    
    const results: AudioAnnotationResult[] = [];
    let successCount = 0;
    let errorCount = 0;
    let totalAudioDuration = 0;

    for (const pair of pairs) {
      try {
        const audioPath = `${audioDirectory}/client_turn_${pair.client_turn_id}.wav`;
        
        if (!fs.existsSync(audioPath)) {
          console.warn(`Audio file not found: ${audioPath}`);
          errorCount++;
          continue;
        }

        const result = await this.annotateWithAudio(pair, charteId, audioPath);
        results.push(result);
        successCount++;
        totalAudioDuration += result.audio_duration_s;

        // Rate limiting : 3 req/sec max (OpenAI limit)
        await this.sleep(350);

      } catch (error) {
        console.error(`Error on pair ${pair.pair_id}:`, error);
        errorCount++;
      }
    }

    // Calculer co√ªt estim√©
    const costPerMinute = 0.06; // $0.06 / minute audio
    const totalCost = (totalAudioDuration / 60) * costPerMinute;

    return {
      success: successCount,
      errors: errorCount,
      results,
      totalCost,
      totalDuration: totalAudioDuration
    };
  }

  private static sleep(ms: number): Promise<void> {
    return new Promise(resolve => setTimeout(resolve, ms));
  }
}
```

---

## üé® Interface UI

### AudioTestingPanel.tsx

**Composant pour tester avec audio** :

```typescript
import React, { useState } from 'react';
import {
  Card,
  CardContent,
  Typography,
  Button,
  Box,
  Alert,
  LinearProgress,
  FormControl,
  FormLabel,
  Select,
  MenuItem,
  TextField
} from '@mui/material';
import MicIcon from '@mui/icons-material/Mic';
import { OpenAIAudioAnnotationService } from '@/services/OpenAIAudioAnnotationService';

export const AudioTestingPanel: React.FC = () => {
  const [charteId, setCharteId] = useState('CharteY_B_v1.0.0');
  const [sampleSize, setSampleSize] = useState(10);
  const [audioDirectory, setAudioDirectory] = useState('/audio/extracted');
  const [testing, setTesting] = useState(false);
  const [progress, setProgress] = useState(0);
  const [result, setResult] = useState<any>(null);

  const handleTest = async () => {
    setTesting(true);
    setProgress(0);

    try {
      // R√©cup√©rer √©chantillon paires
      const pairs = await fetchRandomPairs(sampleSize);
      
      // Annoter avec audio
      const batchResult = await OpenAIAudioAnnotationService.annotateBatchWithAudio(
        pairs,
        charteId,
        audioDirectory
      );

      setResult(batchResult);

    } catch (error) {
      console.error('Error testing with audio:', error);
      alert('Erreur lors du test audio');
    } finally {
      setTesting(false);
    }
  };

  return (
    <Card>
      <CardContent>
        <Box sx={{ display: 'flex', alignItems: 'center', gap: 1, mb: 2 }}>
          <MicIcon color="primary" />
          <Typography variant="h6">
            Test Annotation Audio
          </Typography>
        </Box>

        <Alert severity="info" sx={{ mb: 3 }}>
          Mode Audio : Le LLM analysera les fichiers audio complets (texte + ton + √©motion).
          Co√ªt estim√© : ~$0.02 par paire.
        </Alert>

        {/* Configuration */}
        <FormControl fullWidth sx={{ mb: 2 }}>
          <FormLabel>Charte</FormLabel>
          <Select value={charteId} onChange={(e) => setCharteId(e.target.value)}>
            <MenuItem value="CharteY_A_v1.0.0">Charte A - Minimaliste</MenuItem>
            <MenuItem value="CharteY_B_v1.0.0">Charte B - Enrichie</MenuItem>
            <MenuItem value="CharteY_C_v1.0.0">Charte C - Binaire</MenuItem>
          </Select>
        </FormControl>

        <TextField
          fullWidth
          type="number"
          label="Nombre de paires"
          value={sampleSize}
          onChange={(e) => setSampleSize(parseInt(e.target.value))}
          sx={{ mb: 2 }}
        />

        <TextField
          fullWidth
          label="R√©pertoire audio"
          value={audioDirectory}
          onChange={(e) => setAudioDirectory(e.target.value)}
          sx={{ mb: 3 }}
        />

        {/* Bouton Test */}
        <Button
          fullWidth
          variant="contained"
          size="large"
          onClick={handleTest}
          disabled={testing}
          startIcon={<MicIcon />}
        >
          {testing ? 'Test en cours...' : 'Lancer Test Audio'}
        </Button>

        {/* Progress */}
        {testing && (
          <Box sx={{ mt: 2 }}>
            <LinearProgress variant="determinate" value={progress} />
            <Typography variant="caption" sx={{ mt: 1 }}>
              Annotation en cours... {progress}%
            </Typography>
          </Box>
        )}

        {/* R√©sultats */}
        {result && (
          <Box sx={{ mt: 3 }}>
            <Typography variant="h6" gutterBottom>
              R√©sultats
            </Typography>
            <Alert severity="success">
              ‚úÖ {result.success} paires annot√©es avec succ√®s
              ‚ùå {result.errors} erreurs
              üí∞ Co√ªt total : ${result.totalCost.toFixed(2)}
              ‚è±Ô∏è Dur√©e audio totale : {result.totalDuration.toFixed(1)}s
            </Alert>
          </Box>
        )}
      </CardContent>
    </Card>
  );
};
```

---

## üí∞ Co√ªts & Performances

### Tarification GPT-4o Audio

**Input Audio** : $0.06 / minute  
**Input Text** : $2.50 / 1M tokens  
**Output Text** : $10.00 / 1M tokens

### Calcul Co√ªt 901 Paires

**Hypoth√®ses** :
- Dur√©e moyenne turn client : 15-30 secondes (moyenne 22.5s)
- Prompt texte : ~200 tokens input, ~50 tokens output

**Co√ªts** :
```
Audio input : 
  901 paires √ó 22.5s = 20,272s = 338 minutes
  338 min √ó $0.06/min = $20.28

Text input :
  901 √ó 200 tokens √ó $2.50/1M = $0.45

Text output :
  901 √ó 50 tokens √ó $10/1M = $0.45

TOTAL : $21.18 pour tout le corpus
Soit : $0.024 par paire
```

**C'est tr√®s abordable !** üí∞

---

### Performances Temps

**Vitesse annotation** :
- 1 paire = ~2-4 secondes (API call + processing)
- Rate limit = 3 req/sec max
- 901 paires = ~5-10 minutes total (batch)

**Extraction audio** :
- 1 paire = ~0.5 secondes (ffmpeg)
- 901 paires = ~8 minutes total

**TOTAL : ~15-20 minutes** pour annoter tout le corpus en mode audio

---

## üî¨ Hypoth√®se Scientifique H5

**H5** : Un LLM multimodal (texte+audio) atteint un accord inter-annotateurs similaire √† un humain √©coutant l'audio complet.

### Pr√©dictions

```
Œ∫(LLM_audio, Humain_audio)      = 0.70-0.80 (hypoth√®se)
Œ∫(LLM_texte, Humain_texte_only) = 0.75-0.85 (d√©j√† mesur√© Sprint 4)
Œ∫(Humain_audio, Humain_texte)   = 0.40-0.60 (impact prosodie)

Conclusion esp√©r√©e :
‚Üí LLM_audio capture la prosodie aussi bien que l'humain !
‚Üí LLM_audio > LLM_texte quand compar√© √† Humain_audio
```

---

## üìã Workflow Complet

### √âtape 1 : Pr√©paration Audio (One-Time)

**Localiser fichiers audio originaux** :
```bash
/audio/original/
  call_001.wav
  call_002.wav
  ...
```

**Extraire tous les segments** :
```typescript
await AudioExtractionService.extractAllAudio(
  '/audio/original',
  '/audio/extracted'
);

// R√©sultat :
// /audio/extracted/
//   call_001/
//     client_turn_123.wav
//     conseiller_turn_124.wav
//     client_turn_125.wav
//     ...
```

**Dur√©e** : ~10-15 minutes pour 901 paires

---

### √âtape 2 : Annotation Batch

**Annoter √©chantillon (ex: 50 paires)** :
```typescript
const pairs = await fetchRandomPairs(50);

const result = await OpenAIAudioAnnotationService.annotateBatchWithAudio(
  pairs,
  'CharteY_B_v1.0.0',
  '/audio/extracted/call_001'
);

console.log(`
  Success: ${result.success}
  Cost: $${result.totalCost}
  Duration: ${result.totalDuration}s
`);
```

**Dur√©e** : ~2-3 minutes pour 50 paires  
**Co√ªt** : ~$1.20 pour 50 paires

---

### √âtape 3 : Comparaison Kappa

**Utiliser KappaComparator** :
```typescript
// S√©lection UI
Annotateur 1 : LLM Audio (GPT4o-audio_CharteY_B_v1.0.0)
Annotateur 2 : Thomas (Texte + Audio)

// R√©sultat
Œ∫ = 0.75
Accuracy = 80%
D√©saccords = 10 / 50

Interpr√©tation : Bon accord !
‚Üí LLM audio capture bien la prosodie
```

---

### √âtape 4 : Analyse D√©saccords

**Comparer avec version texte** :
```typescript
// D√©saccord audio
Pair 3187 : 
  LLM_audio = POSITIF
  Humain = POSITIF
  ‚úÖ Accord (LLM capte le ton enthousiaste)

// M√™me paire texte
Pair 3187 :
  LLM_texte = NEUTRE
  Humain = POSITIF
  ‚ùå D√©saccord (LLM rate le ton)

‚Üí Prouve que LLM audio > LLM texte sur cas prosodiques
```

---

## üéØ Checklist Impl√©mentation

### Phase 1 : Extraction Audio (1h)

- [ ] Installer ffmpeg : `npm install fluent-ffmpeg`
- [ ] Cr√©er `AudioExtractionService.ts`
- [ ] Impl√©menter `extractPairAudio()`
- [ ] Impl√©menter `extractCallAudio()`
- [ ] Impl√©menter `extractAllAudio()`
- [ ] Tester extraction sur 1 call
- [ ] V√©rifier qualit√© audio (16kHz mono)
- [ ] Batch extraction 901 paires

### Phase 2 : Annotation Audio (2h)

- [ ] Installer OpenAI SDK : `npm install openai`
- [ ] Cr√©er `OpenAIAudioAnnotationService.ts`
- [ ] Impl√©menter `annotateWithAudio()`
- [ ] Impl√©menter `buildAudioPrompt()`
- [ ] Impl√©menter `annotateBatchWithAudio()`
- [ ] Tester sur 5 paires manuellement
- [ ] V√©rifier parsing r√©ponses
- [ ] Tester rate limiting

### Phase 3 : Interface UI (1h)

- [ ] Cr√©er `AudioTestingPanel.tsx`
- [ ] Int√©grer dans `Level0Interface.tsx`
- [ ] Impl√©menter s√©lection charte
- [ ] Impl√©menter s√©lection √©chantillon
- [ ] Impl√©menter progress bar
- [ ] Afficher r√©sultats batch
- [ ] Tester UX compl√®te

### Phase 4 : Comparaisons (30min)

- [ ] Annoter 50 paires audio
- [ ] Comparer Œ∫(LLM_audio, Humain_audio)
- [ ] Comparer avec Œ∫(LLM_texte, Humain_audio)
- [ ] Analyser d√©saccords sp√©cifiques audio
- [ ] Documenter r√©sultats H5

---

## üìä R√©sultats Attendus

### Tableau Comparatif Final

| Comparaison | Kappa | Interpr√©tation |
|-------------|-------|----------------|
| LLM_texte vs Humain_texte | 0.82 | Excellent (m√™me modalit√©) |
| LLM_audio vs Humain_audio | **0.75** | Bon (H5 valid√©e !) |
| LLM_texte vs Humain_audio | 0.25 | Faible (conflit modalit√©) |
| Humain_texte vs Humain_audio | 0.45 | Mod√©r√© (impact prosodie) |

### Conclusion Scientifique

**Les LLM multimodaux (audio) r√©duisent de 67% l'√©cart d'accord caus√© par l'absence de prosodie** :
```
Gap LLM_texte : 0.82 - 0.25 = 0.57
Gap LLM_audio : 0.82 - 0.75 = 0.07

R√©duction : (0.57 - 0.07) / 0.57 = 88%
```

**Impact Pratique** :
- LLM audio utilisable pour pr√©-annotation sur donn√©es audio
- Pr√©cision proche humain (75% vs 82%)
- Co√ªt acceptable (~$0.02/paire)

---

## üöÄ Extensions Futures

### Extension 1 : Multi-Models

**Tester autres mod√®les audio** :
- Gemini 1.5 Pro Audio
- Whisper + Sentiment Analysis
- Comparer performances/co√ªts

---

### Extension 2 : Features Prosodiques

**Extraire features audio explicites** :
```typescript
{
  pitch_mean: 180,        // Hz
  pitch_variance: 45,
  energy_mean: 0.65,
  speech_rate: 4.2,       // syllabes/sec
  pauses_count: 2,
  emotion_detected: 'joie'
}
```

**Ajouter au prompt LLM** ‚Üí Am√©liorer pr√©cision

---

### Extension 3 : Active Learning

**Identifier paires difficiles** :
```
Si LLM_texte ‚â† LLM_audio
‚Üí Paire d√©pend de prosodie
‚Üí Priorit√© annotation humaine audio
```

---

## üìö R√©f√©rences

**GPT-4o Audio** :
- Documentation : https://platform.openai.com/docs/guides/audio
- Pricing : https://openai.com/pricing
- Mod√®le : gpt-4o-audio-preview

**ffmpeg** :
- Documentation : https://ffmpeg.org/
- Node.js wrapper : fluent-ffmpeg

**Prosodie & Annotation** :
- Busso et al. (2008). "IEMOCAP: Interactive emotional dyadic motion capture database"
- Eyben et al. (2010). "Opensmile: the munich versatile and fast open-source audio feature extractor"

---

**Document cr√©√©** : 2025-12-17  
**Version** : 1.0  
**Auteur** : Claude & Thomas  
**Sprint** : Sprint 4+ Audio Extension
